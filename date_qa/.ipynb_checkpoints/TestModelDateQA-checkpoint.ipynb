{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5bdb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\GP\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP i : 0\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDlzbQCKdGziwPeQyVEFxTN3vOuudYqMDI&cx=53282e4da3bc047e2&q=%0A%0AWhat+is+the+birthdate+of+Lawrence+Kip%3F&num=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 225\u001b[0m\n\u001b[0;32m    223\u001b[0m question \u001b[38;5;241m=\u001b[39m unidecode(question)\n\u001b[0;32m    224\u001b[0m task \u001b[38;5;241m=\u001b[39m model_data_input\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 225\u001b[0m CONTEXT, combined_set \u001b[38;5;241m=\u001b[39m combine_wikipedia_titles_and_sections(question)\n\u001b[0;32m    227\u001b[0m response \u001b[38;5;241m=\u001b[39m answer_question(question, CONTEXT)\n\u001b[0;32m    229\u001b[0m end_date \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[8], line 104\u001b[0m, in \u001b[0;36mcombine_wikipedia_titles_and_sections\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_wikipedia_titles_and_sections\u001b[39m(question):\n\u001b[1;32m--> 104\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m search_google(question)\n\u001b[0;32m    105\u001b[0m     wiki_items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m search_results :\n",
      "Cell \u001b[1;32mIn[8], line 81\u001b[0m, in \u001b[0;36msearch_google\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     73\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m: API_KEY,\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcx\u001b[39m\u001b[38;5;124m'\u001b[39m: CX,\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust number of results as needed\u001b[39;00m\n\u001b[0;32m     78\u001b[0m }\n\u001b[0;32m     79\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 81\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Will raise an HTTPError for bad responses\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDlzbQCKdGziwPeQyVEFxTN3vOuudYqMDI&cx=53282e4da3bc047e2&q=%0A%0AWhat+is+the+birthdate+of+Lawrence+Kip%3F&num=2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, LongformerTokenizer, LongformerForMultipleChoice,BertTokenizer, BertForQuestionAnswering,LongformerTokenizer, LongformerForQuestionAnswering\n",
    "import torch\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "#********************************* METHODS ***********************************************************\n",
    "\n",
    "# Function to merge named entities with commas (e.g., \"Kenton, Ohio\")\n",
    "def merge_named_entities(doc):\n",
    "    new_sentence = []\n",
    "    skip_count = 0\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if skip_count > 0:\n",
    "            skip_count -= 1\n",
    "            continue\n",
    "\n",
    "        # Check if the current token is a named entity followed by a comma and another named entity\n",
    "        if token.ent_type_ and i < len(doc) - 2 and doc[i + 1].text == ',' and doc[i + 2].ent_type_ == token.ent_type_:\n",
    "            # Merge current entity and the one after the comma\n",
    "            new_sentence.append(f\"{token.text} {doc[i + 2].text}\")\n",
    "            skip_count = 2  # Skip the next two tokens (comma and next entity)\n",
    "        else:\n",
    "            new_sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(new_sentence)\n",
    "    \n",
    "def keyword_generator(sentence):\n",
    "\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Remove stopwords from the sentence\n",
    "    filtered_sentence = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() not in stop_words:\n",
    "            filtered_sentence.append(token.text)\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    sentence = \" \".join(filtered_sentence)\n",
    "\n",
    "    # Merge named entities connected by commas\n",
    "    processed_sentence = merge_named_entities(doc)\n",
    "\n",
    "    # Process the new sentence with merged entities\n",
    "    doc = nlp(processed_sentence)\n",
    "\n",
    "    # Extract noun phrases\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "    # Remove stopwords from noun phrases and filter by first uppercase character or numbers\n",
    "    filtered_noun_phrases = []\n",
    "    for np in noun_phrases:\n",
    "        filtered_words = [word for word in np.split() if word.lower() not in stop_words]\n",
    "        filtered_np = ' '.join(filtered_words)\n",
    "    \n",
    "        # Check if the first character is uppercase or a digit\n",
    "        if filtered_np and (filtered_np[0].isupper() or filtered_np[0].isdigit()):\n",
    "            filtered_noun_phrases.append(filtered_np)\n",
    "\n",
    "    return ', '.join([x for x in filtered_noun_phrases])\n",
    "\n",
    "\n",
    "def search_google(query):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': API_KEY,\n",
    "        'cx': CX,\n",
    "        'q': query,\n",
    "        'num': 2  # Adjust number of results as needed\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    response.raise_for_status()  # Will raise an HTTPError for bad responses\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_article_sections(title) :\n",
    "    # Fetch the HTML content of the Wikipedia article\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title}\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    sections = {}\n",
    "    for section in soup.find_all(\"h2\"):\n",
    "        if (p_tag := section.find_next(\"p\")) is not None:\n",
    "            sections[section.text] = p_tag.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def combine_wikipedia_titles_and_sections(question):\n",
    "\n",
    "    search_results = search_google(question)\n",
    "    wiki_items = []\n",
    "    if \"items\" in search_results :\n",
    "        wiki_items = search_results[\"items\"]\n",
    "    else :  \n",
    "        keywords_transformers = keyword_generator(question)\n",
    "        search_results = search_google(keywords_transformers)\n",
    "        if \"items\" in search_results :\n",
    "            wiki_items = search_results[\"items\"]\n",
    "\n",
    "    CONTEXT = \"\"\n",
    "    combined_set = []\n",
    "    \n",
    "    for element in wiki_items :\n",
    "        wiki_url = element['link']\n",
    "        title = wiki_url.replace('https://en.wikipedia.org/wiki/', '')\n",
    "        combined_set.append(title)\n",
    "        sections_title = get_article_sections(title)\n",
    "        CONTEXT += title+\" :\\n\" \n",
    "        for h2 in sections_title :\n",
    "            CONTEXT += h2+\"\\n\"+sections_title[h2]+\"\\n\\n\"\n",
    "\n",
    "    return CONTEXT, combined_set\n",
    "\n",
    "\n",
    "def qa_model_hugging_face(question, context):\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the input without token_type_ids (not required for RoBERTa)\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Perform inference (get the start and end logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract start and end logits\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Convert the input_ids to tokens\n",
    "    input_ids = inputs['input_ids']\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "    # Get the most probable start and end positions\n",
    "    start_index = torch.argmax(start_scores, dim=1).item()  # Extracts the most probable index\n",
    "    end_index = torch.argmax(end_scores, dim=1).item() + 1  # Extracts the most probable index for the end\n",
    "\n",
    "    # Convert tokens to answer\n",
    "    answer_tokens = all_tokens[start_index:end_index]\n",
    "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def answer_question(question: str, context: str) -> str:\n",
    "    # Tokenize input with padding and truncation for long context\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=4096)\n",
    "    \n",
    "        # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the start and end scores\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Get the most likely beginning and end of the answer\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "    \n",
    "    # Convert tokens to the final answer\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index]))\n",
    "    \n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "\n",
    "# ***********************************************************************************************************************\n",
    "\n",
    "df = pd.read_csv(\"../model_sample_qa.csv\")[['date','task', 'challenge', 'reference']]\n",
    "date_df = df[df['task'] == 'date_qa'].reset_index(drop=True)\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Your Google Custom Search Engine credentials\n",
    "API_KEY = 'AIzaSyAAYkLy46XcoOV_gPFDmY4WHcfC0XNgX-I'\n",
    "CX='53282e4da3bc047e2'\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model_data_input = date_df.copy()\n",
    "model_data_input[\"result_model\"] = None\n",
    "model_data_input[\"Time_elapsed\"] = None\n",
    "model_data_input['Titles'] = None\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"STEP i :\", str(i))\n",
    "    start_date = time.time()\n",
    "    question = model_data_input.loc[i, \"challenge\"]\n",
    "    question = unidecode(question)\n",
    "    task = model_data_input.loc[i, \"task\"]\n",
    "    CONTEXT, combined_set = combine_wikipedia_titles_and_sections(question)\n",
    "\n",
    "    response = answer_question(question, CONTEXT)\n",
    "    \n",
    "    end_date = time.time()\n",
    "    model_data_input.loc[i, \"Time_elapsed\"] = end_date - start_date\n",
    "    model_data_input.loc[i, \"result_model\"] = response\n",
    "    model_data_input.loc[i, \"Titles\"] = ', '.join([x for x in combined_set])\n",
    "\n",
    "\n",
    "model_data_input.to_csv(\"model_predict_date_qa_allenai.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c2a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
